{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffd24e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1380d20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('ADB_args.pickle', 'rb') as f:\n",
    "    args = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13fae338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "LibMR not installed or libmr.so not found\n",
      "Install libmr: cd libMR/; ./compile.sh\n"
     ]
    }
   ],
   "source": [
    "from configs.base import ParamManager\n",
    "from dataloaders.base import DataManager\n",
    "from backbones.base import ModelManager\n",
    "from methods import method_map\n",
    "from utils.functions import save_results\n",
    "import logging\n",
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77406898",
   "metadata": {},
   "outputs": [],
   "source": [
    "from backbones.bert import BERT, BERT_Norm, BERT_MixUp, BERT_SEG, BERT_Disaware, BERT_DOC\n",
    "backbones_map = {\n",
    "                    'bert': BERT, \n",
    "                    'bert_norm': BERT_Norm,\n",
    "                    'bert_mixup': BERT_MixUp,\n",
    "                    'bert_seg': BERT_SEG,\n",
    "                    'bert_disaware': BERT_Disaware,\n",
    "                    'bert_doc': BERT_DOC\n",
    "                }\n",
    "\n",
    "class ModelManager:\n",
    "\n",
    "    def __init__(self, args, data, logger_name = 'Detection'):\n",
    "        \n",
    "        self.logger = logging.getLogger(logger_name)\n",
    "        if args.backbone.startswith('bert'):\n",
    "            self.model = self.set_model(args, 'bert')\n",
    "            self.optimizer, self.scheduler = self.set_optimizer(self.model, data.dataloader.num_train_examples, args.train_batch_size, \\\n",
    "                args.num_train_epochs, args.lr, args.warmup_proportion) \n",
    "    \n",
    "    def set_optimizer(self, model, num_train_examples, train_batch_size, num_train_epochs, lr, warmup_proportion):\n",
    "        num_train_optimization_steps = int(num_train_examples / train_batch_size) * num_train_epochs\n",
    "\n",
    "        param_optimizer = list(model.named_parameters())\n",
    "        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "        \n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr = lr, correct_bias=False)\n",
    "        num_warmup_steps= int(num_train_examples * num_train_epochs * warmup_proportion / train_batch_size)\n",
    "        \n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                    num_warmup_steps=num_warmup_steps,\n",
    "                                                    num_training_steps=num_train_optimization_steps)\n",
    "        \n",
    "        return optimizer, scheduler\n",
    "    \n",
    "    def set_model(self, args, pattern):\n",
    "        backbone = backbones_map[args.backbone]\n",
    "        print(backbone)\n",
    "        args.device = self.device = torch.device('cuda:%d' % int(args.gpu_id) if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        if pattern == 'bert':\n",
    "            model = backbone.from_pretrained('bert-base-uncased', args = args) \n",
    "            if args.freeze_backbone_parameters:\n",
    "                self.logger.info('Freeze all parameters but the last layer for efficiency')\n",
    "                model = freeze_bert_parameters(model)\n",
    "        model.to(self.device)\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9bc0a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataManager(args, logger_name = args.logger_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d72108e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'backbones.bert.BERT'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BERT: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BERT from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BERT from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BERT were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'dense.bias', 'dense.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "model = ModelManager(args, data, logger_name = args.logger_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204c73e1",
   "metadata": {},
   "source": [
    "위에 두개를 실행 후 \n",
    "```python\n",
    "def run(args, data, model, logger)\n",
    "```\n",
    "요고를 실행 하면 밑으로\n",
    "\n",
    "``` methods/__init__.py ```\n",
    "```python\n",
    "\n",
    "method_map = {\n",
    "                'ADB': ADBManager, \n",
    "                'DA-ADB': ADBManager, \n",
    "                'MSP': MSPManager, \n",
    "                'DeepUnk':DeepUnkManager, \n",
    "                'LOF': DeepUnkManager, \n",
    "                'DOC': DOCManager, \n",
    "                'OpenMax': OpenMaxManager, \n",
    "                'MixUp': MixUpManager,\n",
    "                'SEG': SEGManager\n",
    "            }\n",
    "\n",
    "```\n",
    "```run.py```\n",
    "```python\n",
    "def run(args, data, model, logger):\n",
    "    method_manager = method_map[args.method]\n",
    "    method = method_manager(args, data, model, logger_name = args.logger_name)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "461b91c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ADB'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6364ed8",
   "metadata": {},
   "source": [
    "method는 ```methods/ADB/manager.py``` 파일의 ```calss ADBManager``` 에서 실행됨\n",
    "\n",
    "```python\n",
    "class ADBManager:\n",
    "    print(\"start\")\n",
    "    \n",
    "    def __init__(self, args, data, model, logger_name = 'Detection'):\n",
    "        print(\"ADB init\")\n",
    "        self.logger = logging.getLogger(logger_name)\n",
    "        \n",
    "        pretrain_model = PretrainManager(args, data, model)\n",
    "        self.model = pretrain_model.model\n",
    "        self.centroids = pretrain_model.centroids\n",
    "        self.pretrain_best_eval_score = pretrain_model.best_eval_score\n",
    "```\n",
    "```methods/ADB/pretrain.py``` 의 ```clase PretrainManager```를 살펴보자\n",
    "밑으로는 ```def __init__``` 내용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "63bd7562",
   "metadata": {},
   "outputs": [],
   "source": [
    "PretrainManager_model = model.model\n",
    "PretrainManager_optimizer = model.optimizer\n",
    "PretrainManager_scheduler = model.scheduler\n",
    "PretrainManager_device = model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dd38ede4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BERT(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (activation): ReLU()\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=19, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PretrainManager_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "732eb1be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PretrainManager_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "066a4dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "PretrainManager_train_dataloader = data.dataloader.train_labeled_loader\n",
    "PretrainManager_eval_dataloader = data.dataloader.eval_loader\n",
    "PretrainManager_test_dataloader = data.dataloader.test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a2be4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from losses.CosineFaceLoss import CosineFaceLoss\n",
    "from torch import nn \n",
    "loss_map = {\n",
    "                'CrossEntropyLoss': nn.CrossEntropyLoss(), \n",
    "                'Binary_CrossEntropyLoss': nn.BCELoss(),\n",
    "                'CosineFaceLoss': CosineFaceLoss()\n",
    "            }\n",
    "\n",
    "PretrainManager_loss_fct = loss_map[args.loss_fct]  \n",
    "PretrainManager_centroids = None\n",
    "PretrainManager_best_eval_score = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3be87784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, 'bert')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.pretrain, args.backbone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d542940",
   "metadata": {},
   "source": [
    "if 문에서 ```train_plain```을 실행하게 됨\n",
    "```python\n",
    "        if args.pretrain or (not os.path.exists(args.model_output_dir)):\n",
    "            print(\"pretrainmanager init if\")\n",
    "            self.logger.info('Pre-training Begin...')\n",
    "\n",
    "            if args.backbone == 'bert_disaware':\n",
    "                print(\"pretrainmanager init bert_disaware\")\n",
    "                self.train_disaware(args, data)\n",
    "            else:\n",
    "                print(\"pretrainmanager init else bert_disaware\")\n",
    "                self.train_plain(args, data)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3687dd7",
   "metadata": {},
   "source": [
    "```python\n",
    "class pretrainManager \n",
    "def train_plain:\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3863667a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange, tqdm\n",
    "\n",
    "wait = 0\n",
    "best_model = None\n",
    "best_eval_score = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a53e97b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:   0%|                                                                                                                                                                    | 0/17 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "for epoch in trange(int(args.num_train_epochs), desc=\"Epoch\"):\n",
    "    print(epoch)\n",
    "    PretrainManager_model.train()\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    for step, batch in enumerate(tqdm(PretrainManager_train_dataloader, desc=\"Iteration\")):\n",
    "        batch = tuple(t.to(PretrainManager_device) for t in batch)\n",
    "        input_ids, input_mask, segment_ids, label_ids = batch\n",
    "        \n",
    "        with torch.set_grad_enabled(True):\n",
    "            loss = PretrainManager_model(input_ids, segment_ids, input_mask, label_ids, mode = \"train\", loss_fct = PretrainManager_loss_fct)\n",
    "            \n",
    "        break\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c694a53d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,\n",
       " torch.Size([128, 55]),\n",
       " torch.Size([128, 55]),\n",
       " torch.Size([128, 55]),\n",
       " torch.Size([128]),\n",
       " tensor([  101,  7632,  1010,  1045,  2572,  2006, 10885,  1999,  3577,  1998,\n",
       "          2619, 10312,  2026,  4524,  2007,  2026,  3042,  1998, 15882,  2007,\n",
       "          5329,  1998,  2673,  1012,  2064,  2017,  3796,  2009, 17306,  2361,\n",
       "          1998,  2059,  1045,  2215,  2000,  2344,  1037,  2047,  2028,  2036,\n",
       "         17306,  2361,  1012,   102,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0]),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([13, 10, 14,  1,  1, 14,  6, 16,  4, 12,  0,  4,  0, 11, 10, 15, 14,  0,\n",
       "          8,  1,  1, 16,  5,  9,  1, 17,  3, 11,  4,  5,  7, 15, 10, 11, 12, 14,\n",
       "         16,  7,  6, 10, 13, 18,  3,  7, 16, 11, 12, 13,  5,  4,  1,  0,  2,  7,\n",
       "          1,  3, 17,  2, 10,  9, 17,  4, 15,  2,  9, 15, 12, 13,  0, 15, 17,  3,\n",
       "          9, 11,  9,  0,  2,  1,  1, 13, 10, 12, 14,  5, 14,  5,  7, 15,  1, 13,\n",
       "         10, 15, 10,  6, 10,  7, 12, 10, 18, 10,  4,  2, 11,  5,  3, 13, 18, 10,\n",
       "          0,  5,  4, 17, 10, 14, 11,  4, 10, 14,  0,  4,  5, 14, 10, 16, 10, 11,\n",
       "          5,  1]))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch), batch[0].shape, batch[1].shape, batch[2].shape, batch[3].shape, batch[0][0], batch[1][0], batch[2][0], batch[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923ab8f7",
   "metadata": {},
   "source": [
    "```backbones/bert.py```에 ```class BERT```의 ```forward```를 직접해봄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "140926d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_type_ids = segment_ids\n",
    "attention_mask = input_mask\n",
    "labels = label_ids\n",
    "\n",
    "outputs = PretrainManager_model.bert(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "143eca8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_layer_12 = outputs.hidden_states\n",
    "pooled_output = outputs.pooler_output\n",
    "pooled_output = encoded_layer_12[-1].mean(dim=1)\n",
    "\n",
    "pooled_output = PretrainManager_model.dense(encoded_layer_12[-1].mean(dim=1))\n",
    "pooled_output = PretrainManager_model.activation(pooled_output)\n",
    "pooled_output = PretrainManager_model.dropout(pooled_output)\n",
    "\n",
    "logits = PretrainManager_model.classifier(pooled_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4c7bf7db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 19]),\n",
       " tensor([ 0.0128,  0.0217,  0.0274, -0.0431, -0.0917,  0.0591,  0.0121, -0.1554,\n",
       "          0.0351, -0.0333, -0.0211, -0.0598,  0.0372, -0.0200, -0.1376, -0.0256,\n",
       "         -0.0008,  0.0446,  0.0555], device='cuda:3', grad_fn=<SelectBackward0>))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape, logits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d1186bca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.9441, device='cuda:3', grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_ce = PretrainManager_loss_fct(logits, labels)\n",
    "loss_ce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01af9352",
   "metadata": {},
   "source": [
    "# class ModelManager:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d9c3b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from backbones.bert import BERT, BERT_Norm, BERT_MixUp, BERT_SEG, BERT_Disaware, BERT_DOC\n",
    "backbones_map = {\n",
    "                    'bert': BERT, \n",
    "                    'bert_norm': BERT_Norm,\n",
    "                    'bert_mixup': BERT_MixUp,\n",
    "                    'bert_seg': BERT_SEG,\n",
    "                    'bert_disaware': BERT_Disaware,\n",
    "                    'bert_doc': BERT_DOC\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d6ee6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from backbones.utils import freeze_bert_parameters\n",
    "\n",
    "def set_model(args, pattern):\n",
    "    backbone = backbones_map[args.backbone]\n",
    "    args.device = device = torch.device('cuda:%d' % int(args.gpu_id) if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    if pattern == 'bert':\n",
    "        # cache_dir = \"cache\",이거 지우니깐 되네... 왜?? 3090 머시기 에러 있음\n",
    "        model = backbone.from_pretrained('bert-base-uncased', args = args) \n",
    "        if args.freeze_backbone_parameters:\n",
    "            \n",
    "            model = freeze_bert_parameters(model)\n",
    "    model.to(device)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8463fac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BERT: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BERT from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BERT from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BERT were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['dense.bias', 'dense.weight', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = set_model(args, 'bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "780870f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "def set_optimizer(model, num_train_examples, train_batch_size, num_train_epochs, lr, warmup_proportion):\n",
    "    num_train_optimization_steps = int(num_train_examples / train_batch_size) * num_train_epochs\n",
    "\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr = lr, correct_bias=False)\n",
    "    num_warmup_steps= int(num_train_examples * num_train_epochs * warmup_proportion / train_batch_size)\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps=num_warmup_steps,\n",
    "                                                num_training_steps=num_train_optimization_steps)\n",
    "\n",
    "    return optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8b1665f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer, scheduler = set_optimizer(model, data.dataloader.num_train_examples, args.train_batch_size, \\\n",
    "                args.num_train_epochs, args.lr, args.warmup_proportion) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97078b33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2284bb9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91a92703",
   "metadata": {},
   "outputs": [],
   "source": [
    "method_manager = method_map[args.method]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079a43cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "method = method_manager(args, data, model, logger_name = args.logger_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
